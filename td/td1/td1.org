# -*- coding: utf-8 -*-
#+TITLE: TD1: Introduction et notions fondamentales
#+AUTHOR: Roland Donat
#+EMAIL: roland.donat@univ-ubs.fr
# #+DATE: 

# ==============================================
# Document Configuration
# ======================
# Orgmode
:CONFIG:
#+LANGUAGE: fr
#+OPTIONS: H:3 num:t toc:t \n:nil @:t ::t |:t ^:{} f:t TeX:t author:t d:nil timestamp:nil
#+OPTIONS: html-postamble:nil
#+STARTUP: content 
#+STARTUP: hidestars
#+DRAWERS: CONFIG OPTIONS CACHE MACROS
#+TODO: TODO(t) INPROGRESS(p) | DONE(d)
#+BIND: org-latex-table-scientific-notation "{%s}E{%s}"
:END:

# LaTeX
# -----
# Class parameters
:CONFIG:
#+LaTeX_CLASS: ubs-note
#+LaTeX_CLASS_OPTIONS: [a4paper,twoside,11pt]
#+LATEX_HEADER: \thelang{FR}
#+LATEX_HEADER: \thesubtitle{}
#+LATEX_HEADER: \institution{IUT Vannes}
#+LATEX_HEADER: \course{Classification non supervisée}
#+LATEX_HEADER: \cursus{STID 2 - 2020-2021}
#+LATEX_HEADER: \version{1.0}
:END:
# Packages
:CONFIG:
#+LATEX_HEADER: \usepackage[french]{babel}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage{amssymb}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{amsfonts}
#+LATEX_HEADER: \usepackage{xcolor}
#+LATEX_HEADER: \usepackage{verbatim}
#+LATEX_HEADER: \usepackage{tabularx}
#+LATEX_HEADER: \usepackage{float}
#+LATEX_HEADER: \usepackage{lmodern}
#+LATEX_HEADER: \usepackage{natbib}
#+LATEX_HEADER: \usepackage{subfig}
#+LATEX_HEADER: \usepackage{booktabs}
#+LATEX_HEADER: \usepackage{minted}
:END:
# Layout
:CONFIG:
# Figures path
#+LATEX_HEADER: % Graphics path
#+LATEX_HEADER: \graphicspath{ 
#+LATEX_HEADER:   {./fig/}
#+LATEX_HEADER: }

# Colors
#+LATEX_HEADER: \definecolor{almostwhite}        {rgb}{0.85,0.85,0.85}

# Minted
# To control spaces between minted block
#+LATEX_HEADER: \AtBeginEnvironment{snugshade*}{\vspace{-1.25\FrameSep}}
#+LATEX_HEADER: \AfterEndEnvironment{snugshade*}{\vspace{-2\FrameSep}}
# #+LATEX_HEADER: \usemintedstyle{monokai}
# #+LATEX_HEADER: \renewcommand{\theFancyVerbLine}{\sffamily \footnotesize {\color{EMLogoBlue}\oldstylenums{\arabic{FancyVerbLine}}}}

# Captions
#+LATEX_HEADER: \captionsetup[table]{position=bottom,margin=90pt,font=small,labelfont=bf,labelsep=endash,format=plain}
#+LATEX_HEADER: \captionsetup[figure]{position=bottom,margin=90pt,font=small,labelfont=bf,labelsep=endash,format=plain}
#+LATEX_HEADER: \captionsetup[subfloat]{margin=0pt,font=footnotesize}

# Geometry
#+LATEX_HEADER: \usepackage{geometry}
#+LATEX_HEADER: \geometry{
#+LATEX_HEADER: %  nohead,
#+LATEX_HEADER:   top=2.25cm, 
#+LATEX_HEADER:   bottom=2.25cm, 
#+LATEX_HEADER:  left=2.5cm, 
#+LATEX_HEADER:  right=2.5cm}

#+LATEX_HEADER: \usepackage{setspace}
#+LATEX_HEADER: \onehalfspacing
#+LATEX_HEADER: % Supprime l'indentation
#+LATEX_HEADER: \setlength{\parindent}{0pt}
#+LATEX_HEADER: % Espacement entre les paragraphes
#+LATEX_HEADER: \setlength{\parskip}{2ex}

# List layout
#+LATEX_HEADER: \frenchbsetup{ListOldLayout=true} %FBReduceListSpacing=true,CompactItemize=false}

# References
#+LATEX: \renewcommand*{\refname}{}*
:END:
# LaTeX Compilator
:CONFIG:
#+BEGIN_SRC emacs-lisp :results silent :exports none
(setq org-latex-listings 'minted
      org-latex-minted-options nil ;; '(("frame" "lines")))
      org-latex-pdf-process
      '("xelatex -shell-escape -interaction nonstopmode -output-directory %o %f"
        "bibtex %b"
        "xelatex -shell-escape -interaction nonstopmode -output-directory %o %f"
        "xelatex -shell-escape -interaction nonstopmode -output-directory %o %f"))
#+END_SRC
:END:

# HTML
# ----
:CONFIG:
# Org HTML Macros
#+MACRO: NEWLINE @@latex:\\@@ @@html:<br>@@
#+MACRO: HTMLFONTSIZE @@html:<font size="$2">$1</font>@@
#+MACRO: SUBTITLE @@html:<div class="slidesubtitle">$1</div>@@

# HTML options
# ------------
#+SETUPFILE: https://fniessen.github.io/org-html-themes/org/theme-readtheorg.setup
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="https://roland-donat.github.io/ubs/Charte_graphique/IUT/ubs_iut_vannes.css" />
# #+HTML_HEAD: <link rel="stylesheet" type="text/css" href="./ubs_iut_vannes.css" />
:END:

# Publishing
# ----------
:CONFIG:
#+BEGIN_SRC emacs-lisp :results silent :exports none
;; Define some export options here since in org-publish-project-alist some of them are not taken into account
;; e.g. with-toc nil
(defun my-html-export-options (plist backend)
  (cond 
    ((equal backend 'html)
     (plist-put plist :with-toc t)
     (plist-put plist :section-numbers nil)
     (plist-put plist :with-author t)
     (plist-put plist :with-email t)
     (plist-put plist :with-date t)
     ))
  plist)

(setq org-publish-project-alist
      '(
	
        ("main"
         :base-directory "./"
         :include ("rb_mod_stoch.org")
         :publishing-directory "./"
         :recursive nil
         :publishing-function org-html-publish-to-html
         :preparation-function (lambda () (setq org-export-filter-options-functions '(my-html-export-options)))
         :auto-preamble t
         :html-head  "<link rel='stylesheet' type='text/css' href='edgemind.css' />"
         :htmlized-source 
	 :section-numbers nil
         )
        ("td-html"
         :base-directory "./td/"
         :base-extension "org"
         :publishing-directory "./td"
         :recursive t
         :publishing-function org-html-publish-to-html
         :preparation-function (lambda () (setq org-export-filter-options-functions '(my-html-export-options)))
         :auto-preamble t
         :html-head  "<link rel='stylesheet' type='text/css' href='edgemind.css' />"
         :htmlized-source 
         )

	 ;; pdf
        ("td-pdf"
         :base-directory "./td/"
         :base-extension "org"
         :publishing-directory "./td"
         :recursive t
         :publishing-function org-latex-publish-to-pdf
         )

	 ("td-attach"
	 :base-directory "./td/"
	 :base-extension "xdsl\\|txt\\|csv\\|py\\|png"
         :publishing-directory "./td"
	 :recursive t
	 :publishing-function org-publish-attachment
	 )

	 ("cours-attach"
	 :base-directory "./cours/"
	 :base-extension "pdf\\|xdsl\\|txt\\|csv\\|py"
         :publishing-directory "./cours"
	 :recursive t
	 :publishing-function org-publish-attachment
	 )

        ("projet-html"
         :base-directory "./projet/"
         :base-extension "org"
         :publishing-directory "./projet"
         :recursive t
         :publishing-function org-html-publish-to-html
         :preparation-function (lambda () (setq org-export-filter-options-functions '(my-html-export-options)))
         :auto-preamble t
         :html-head  "<link rel='stylesheet' type='text/css' href='edgemind.css' />"
         :htmlized-source 
         )

	 ("projet-attach"
	 :base-directory "./projet/"
	 :base-extension "xdsl\\|txt\\|csv"
         :publishing-directory "./projet"
	 :recursive t
	 :publishing-function org-publish-attachment
	 )

	 ("css"
         :base-directory "./css/"
         :base-extension "css"
         :publishing-directory "./www/css"
         :publishing-function org-publish-attachment)
	 
	 ;("rb_mod_stoch" :components ("main" "td-pdf" "td-html" "td-attach" "cours-attach" "projet-html" "projet-attach" "css"))
	 ;("rb_mod_stoch" :components ("main" "td-pdf" "td-html" "projet-html"))
	 ("rb_mod_stoch" :components ("main"))

      ))
#+END_SRC
:END:

# ==============================================
# Document starts here
# ====================

#+LATEX: \clearpage

* Préambule

** Crédit

Ce TD reprend la trame du [[https://moodle.univ-ubs.fr/pluginfile.php/360513/mod_resource/content/2/TDR1.pdf][TD de classification non supervisée]] proposé par Mme. Arlette Antoni en
2019-2020. Le TD original était réalisé avec le logiciel =R= tandis que nous allons utiliser le
logiciel =Python=.

** Environnement logiciel

Les travaux se dérouleront sous =Python=. Si vous disposez d'une connexion internet, je vous
recommande d'utiliser une plateforme de /Notebooks/ en ligne telle que
[[https://deepnote.com][=DeepNote=]] sinon vous pouvez
toujours utiliser =Python= en local en installant la distribution [[https://www.anaconda.com/products/individual][=Anaconda=]]. L'important est de vous
assurer que vous utilisez bien la version =3.8= ou plus de =Python=.

* Introduction

L'objectif de ce TD est de mettre en pratique les principales notions vues en cours en les
appliquant sur un jeu de données réel.

Pour ce faire, nous utiliserons une adaptation des données libres [[https://archive.ics.uci.edu/ml/datasets/wine][Wine Data Set]] ne conservant que
les variables quantitatives pour la mise en oeuvre de méthodes de classification non supervisée.

Les données correspondent aux résultats d'analyses chimiques sur des vins issus d'une même région
d'Italie mais produits par trois vignerons différents. L'analyse quantifie la présence des 13
éléments suivants :  
- Alcohol
- Malic acid
- Ash
- Alcalinity of ash
- Magnesium
- Total phenols
- Flavanoids
- Nonflavanoid phenols
- Proanthocyanins
- Color intensity
- Hue
- OD280/OD315 of diluted wines
- Proline

Les données retravaillées à utiliser dans ce TD sont [[https://roland-donat.github.io/cours-class-non-sup/td/td1/wine.csv][disponibles à ici]].

* Modules =Python= utilisés dans ce TD

Dans ce TD, nous utiliserons les modules =Python= suivants :
- =pandas=, pour la manipulation des données ;
- =plotly=, pour les représentations graphiques ;
- =numpy=, pour utiliser des fonctions de calculs numériques "bas niveau", e.g. génération de
  nombres aléatoires ;
- =scipy=, pour utiliser d'autres fonctions de calculs numériques plus "haut niveau", e.g. calcul de
  distances ;
# - =sklearn=, pour faire du /machine learning/.

Ces modules ne sont pas forcément installés dans votre environnement local ou distant de type
=Deepnote=. Vous pouvez donc utiliser la commande =!pip install <nom_module>= pour les installer :
#+attr_latex: :options bgcolor=almostwhite, frame=lines
#+BEGIN_SRC python :results silent :exports code
!pip install pandas
!pip install plotly
!pip install scipy
#+END_SRC
 

Rappel : code d'import des modules :
#+attr_latex: :options bgcolor=almostwhite, frame=lines
#+BEGIN_SRC python :session :tangle td1.py :results silent :exports both
import pandas as pd
import numpy as np
import plotly.express as px
from scipy.spatial.distance import pdist, cdist, squareform
#+END_SRC


* Chargement des données et premières analyses

1. Chargez les données dans votre environnement à l'aide de la fonction =pd.read_csv=. 
2. Faites une rapide analyse exploratoire de vos données :
   - de manière macroscopique avec les méthodes =.head=, =.describe= et =.info=
   - de manière visuelle en représentant la distribution de chaque variable en utilisant la fonction
     =px.box=. 
     Que constatez vous ?
3. Centrez et réduisez les données. Aide : utilisez les méthodes =.mean= et =.std= des =DataFrame=.
4. Calculez la matrice des corrélations linéaires avec la méthode =.corr= des =DataFrame=. Quelles
   sont les caractéristiques remarquables de cette matrice ?
5. Afficher un diagramme en paire et une carte de chaleur des corrélations linéaires.

#+attr_latex: :options bgcolor=almostwhite, frame=lines
#+BEGIN_SRC python :session :tangle td1.py :results silent :exports none
data_path = "https://roland-donat.github.io/cours-class-non-sup/td/td1/wine.csv"
data_df = pd.read_csv(data_path, sep=",")

# Affichage d'informations générales
print(data_df.describe())

print(data_df.info())

px.box(data_df, 
       title="Boxplot de chaque variable des données d'analyse des vins").show()

# Centrage et réduction des données
data_scaled_df = (data_df - data_df.mean()).div(data_df.std())

px.box(data_scaled_df, 
       title="Boxplots des données d'analyse des vins centrées-reduites").show()

# Calcul de la matrice de corrélation
data_scaled_corr_mat = data_scaled_df.corr()
px.imshow(data_scaled_corr_mat, 
          title="Matrice de corrélations linéaires des données d'analyse des vins").show()

px.scatter_matrix(data_scaled_df, 
                  title="Diagramme en paires des données d'analyse des vins").show()

#+END_SRC

* Distance euclidienne

1. Calculez la distance euclidienne entre les deux premiers individus. Aide :
   utilisez la méthode =sum= des =DataFrame=.
2. Calculez la distance euclidienne entre le premier individu et le troisième, puis entre le
   deuxième et le troisième. Que constatez vous ?
3. Calculez la matrice des distances euclidiennes. Aide : utilisez les fonctions =pdist= et
   =squareform= du package =scipy.spatial.distance=. Quelle sera la taille de cette matrice, quelles
   seront ses propriétés remarquables ?
4. Vérifiez que les distances calculées au point 1 grâce à la matrice des distances calculée au
   point 3.

#+attr_latex: :options bgcolor=almostwhite, frame=lines
#+BEGIN_SRC python :session :tangle td1.py :results silent :exports none
# Distances euclidiennes entre les trois premiers individus
d01 = ((data_scaled_df.iloc[0] - data_scaled_df.iloc[1])**2).sum()**(0.5)
d02 = ((data_scaled_df.iloc[0] - data_scaled_df.iloc[2])**2).sum()**(0.5)
d12 = ((data_scaled_df.iloc[1] - data_scaled_df.iloc[2])**2).sum()**(0.5)

# Vérification de l'inégalité triangulaire
print("Vérification de l'inégalité triangulaire !") \
    if (d01 + d12) >= d12 \
    else print("Les mathématiques s'effondrent...")

# Calcul des distances des observations deux à deux sous forme unidimensionnelle
data_dvec_euc = pdist(data_scaled_df, metric="euclidean")
# Mise en forme de matrice carrée
data_dmat_euc = squareform(data_dvec_euc)
# Conversion en DataFrame pour le confort
data_dmat_euc_df = pd.DataFrame(data_dmat_euc,
                                index=data_scaled_df.index,
                                columns=data_scaled_df.index)

data_dmat_euc_df.iloc[0,1] == d01
data_dmat_euc_df.iloc[0,2] == d02
data_dmat_euc_df.iloc[1,2] == d12
#+END_SRC

* Inertie totale

*Dans la suite du TD, les données seront considérées comme étant équipondérées avec un poids de 1
pour chaque observation. Par ailleurs, la distance utilisée sera la distance euclidienne.* 

1. Calculer le centre de gravité du nuage d'observations.
2. Calculer les distances au carré entre les observations et le centre de gravité du nuage. 
3. En déduire l'inertie totale des données.
4. Calculer la somme des variance empirique de chaque variable. Aide : utilisez la méthode =.var= des
   =DataFrame= ? Quels calculs fait exactement la méthode =.var= ? Aurait t-on pu prévoir le résultat
   dans ce cas ?
5. Comparez l'inertie totale avec la somme des variances des variables. 

#+attr_latex: :options bgcolor=almostwhite, frame=lines
#+BEGIN_SRC python :session :tangle td1.py :results silent :exports none
mu_data = data_scaled_df.mean()
d2_data_mu = ((data_scaled_df - mu_data)**2).sum(axis=1)
I_T = d2_data_mu.sum()

data_var_sum = data_scaled_df.var().sum()

# Le rapport entre l'inertie totale et la somme des variances des variables donne
# la taille du jeu de donnée - 1 car :
# - nous sommes dans le cas équipondéré avec poids = 1
# - la méthode var() calcule la variance empirique corrigée 
#   Rappel : variance empirique corrigée = effectif/(effectif-1) * variance empirique
print(I_T/data_var_sum)
#+END_SRC

* Première partition $C_{\text{A}}$

** Construction des classes

Nous allons nous donner une première partition $C_{\text{A}}$ totalement arbitraire consistant à affecter :
- la classe $c_{1}$ aux individus d'indices 0-49 ;
- la classe $c_{2}$ aux individus d'indices 50-99 ;
- la classe $c_{3}$ aux individus d'indices 101-177.

*Rappel :* N'oubliez pas que =Python= indexe les listes, vecteurs, tableaux en commençant à 0 !

1. Ajoutez une nouvelle variable =cls_CA= contenant la classe de chaque individu suivant le schéma
   décrit précédemment.
2. Visualiser le nuage d'individus sur les variables =OD280= et =Alcohol= en faisant apparaître
   votre partition. Aide : utilisez la fonction =px.scatter= en utilisant l'option =color= pour
   colorer les individus en fonction de leur classe.

#+attr_latex: :options bgcolor=almostwhite, frame=lines
#+BEGIN_SRC python :session :tangle td1.py :results silent :exports none
classif_name = "CA"
print("\nPartition {classif_name}\n")
print("-----------------\n")


# Création d'une nouvelle variable initialisée à la valeur c1
data_scaled_df["cls_CA"] = "c1"
# Affectation de la classe 1 aux individus 50-99
data_scaled_df["cls_CA"].iloc[50:100] = "c2"
# Affectation de la classe 2 aux individus restant
data_scaled_df["cls_CA"].iloc[100:] = "c3"

# Visualisation
px.scatter(data_scaled_df,
           x="OD280", y="Alcohol",
           color="cls_CA",
           title=f"OD280 vs Alcohol avec partition {classif_name}").show()

#+END_SRC


** Inertie intra-classe

1. Calculez le centre de gravité de chaque classe. Aide : utilisez la méthode =groupby= des
   =DataFrame= afin de regrouper les données sur la variable de classe et réaliser des traitements
   sur chaque groupe.
2. Calculez l'inertie interne de chaque classe.
3. En déduire l'inertie intra-classe de la partition.
4. En déduire l'inertie inter-classe de la partition.
5. Calculez le % d'inertie expliquée par la partition.
6. Calculez la somme des variances empiriques corrigées des variables au sein de chaque classe.
7. Comparez les inerties internes et le résultat de la question 6.
8. Comment calculer l'inertie intra-classe de la partition à partir de la question 6.

#+attr_latex: :options bgcolor=almostwhite, frame=lines
#+BEGIN_SRC python :session :tangle td1.py :results silent :exports none
# Regroupement des données par classe
data_cls_CA_grp = data_scaled_df.groupby("cls_CA")

# Calcul des moyennes (centres de gravité de chaque classe)
mu_cls_CA = data_cls_CA_grp.mean()

I_cls_CA = pd.Series(0, index=mu_cls_CA.index, name="I_cls_CA")
for cls_CA, data_cls_CA_df in data_cls_CA_grp:
    # Calcul des distances au carré entre chaque individu de la classe et son centre
    d2_data_cls_CA = ((data_cls_CA_df - mu_cls_CA.loc[cls_CA])**2).sum(axis=1)
    # Déduction de l'inertie interne à la classe
    I_cls_CA.loc[cls_CA] = d2_data_cls_CA.sum()

# Inertie intra-classe de la partition
I_W_CA = I_cls_CA.sum()

# Déduction de l'inertie inter-classe
print(f"I_B({classif_name}) = {I_T - I_W_CA}")

# Calcul du % d'inertie expliquée par la partition
PI_CA = 100*(1 - I_W_CA/I_T)

# Calcul de la somme des variances des variables au sein de chaque classe
data_cls_CA_var_sum = data_cls_CA_grp.var().sum(axis=1)

# Le rapport entre l'inertie interne et la variance interne donne
# l'effectif de la classe - 1 car
# - nous sommes dans le cas équipondéré avec poids = 1
# - la méthode var() calcule la variance empirique corrigée 
#   Rappel : variance empirique corrigée = effectif/(effectif-1) * variance empirique
print(I_cls_CA/data_cls_CA_var_sum)

# Effectif de chaque classe - 1
N_cls_CA = data_cls_CA_grp["Alcohol"].count()
# Somme de la somme des variances empiriques corrigées des variables de chaque classe
# pondérée par l'effectif de la classe
I_W_CA_bis = ((N_cls_CA - 1)*data_cls_CA_var_sum).sum()
#+END_SRC


** Inertie inter-classe

1. Calculez le carré des distances entre le centre de gravité des classes et le centre de gravité
   des données.
2. En déduire l'inertie inter-classe de la partition.

#+attr_latex: :options bgcolor=almostwhite, frame=lines
#+BEGIN_SRC python :session :tangle td1.py :results silent :exports none

# Calcul des distances au carré entre les centres de gravité de classe et
# le centre de gravité des données
e2_mu_cls_CA_fact = (mu_cls_CA - mu_data)**2
d2_mu_cls_CA_data = e2_mu_cls_CA_fact.sum(axis=1)
I_B_CA = (N_cls_CA*d2_mu_cls_CA_data).sum()

# Complément lien avec la variance
# Calcul de la variance des variables dans le cas où l'on remplace chaque
# individu par le centre de sa classe
d2_mu_cls_CA_fact = (mu_cls_CA - mu_data)**2
mu_cls_CA_var = (N_cls_CA @ e2_mu_cls_CA_fact)/(len(data_scaled_df) - 1)
mu_cls_CA_var_sum = mu_cls_CA_var.sum()

# == effectif - 1
print(f"Rapport entre I_B({classif_name}) et somme des variances inter = {I_B_CA/mu_cls_CA_var_sum}")
#+END_SRC


* Seconde partition $C_{\text{B}}$

Reprendre les questions de la partie précédente en construisant une partition $C_{\text{B}}$ au
hasard. Pour ce faire, utilisez la fonction =np.ramdom.choice= pour affecter les classes aux
individus. N'hésitez pas à fixer la graine du générateur aléatoire avec la fonction =np.random.seed=
afin de reproduire le "même hasard" d'une exécution à l'autre de votre script. 

#+attr_latex: :options bgcolor=almostwhite, frame=lines
#+BEGIN_SRC python :session :tangle td1.py :results silent :exports none
classif_name = "CB"
print("\nPartition {classif_name}\n")
print("-----------------\n")

# On fixe la graîne du générateur de nombre aléatoire pour 
# reproduire le même "hasard" d'une exécutation à l'autre
np.random.seed(56)

# Initalisation au hasard de la partition
data_scaled_df["cls_CB"] = np.random.choice(["c1", "c2", "c3"],
                                            len(data_scaled_df))

# Visualisation
px.scatter(data_scaled_df,
           x="OD280", y="Alcohol",
           color="cls_CB",
           title=f"OD280 vs Alcohol avec partition {classif_name}").show()

# Regroupement des données par classe
data_cls_CB_grp = data_scaled_df.groupby("cls_CB")

# Calcul des moyennes (centres de gravité de chaque classe)
mu_cls_CB = data_cls_CB_grp.mean()

I_cls_CB = pd.Series(0, index=mu_cls_CB.index, name="I_cls_CB")
for cls_CB, data_cls_CB_df in data_cls_CB_grp:
    # Calcul des distances au carré entre chaque individu de la classe et son centre
    d2_data_cls_CB = ((data_cls_CB_df - mu_cls_CB.loc[cls_CB])**2).sum(axis=1)
    # Déduction de l'inertie interne à la classe
    I_cls_CB.loc[cls_CB] = d2_data_cls_CB.sum()

# Inertie intra-classe de la partition
I_W_CB = I_cls_CB.sum()

# Déduction de l'inertie inter-classe
print(f"I_B({classif_name}) = {I_T - I_W_CB}")

# Calcul du % d'inertie expliquée par la partition
PI_CB = 100*(1 - I_W_CB/I_T)

# Calcul de la somme des variances des variables au sein de chaque classe
data_cls_CB_var_sum = data_cls_CB_grp.var().sum(axis=1)

# Le rapport entre l'inertie interne et la variance interne donne
# l'effectif de la classe - 1 car
# - nous sommes dans le cas équipondéré avec poids = 1
# - la méthode var() calcule la variance empirique corrigée 
#   Rappel : variance empirique corrigée = effectif/(effectif-1) * variance empirique
print(I_cls_CB/data_cls_CB_var_sum)

# Effectif de chaque classe - 1
N_cls_CB = data_cls_CB_grp["Alcohol"].count()
# Somme de la somme des variances empiriques corrigées des variables de chaque classe
# pondérée par l'effectif de la classe
I_W_CB_bis = ((N_cls_CB - 1)*data_cls_CB_var_sum).sum()

# Calcul des distances au carré entre les centres de gravité de classe et
# le centre de gravité des données
e2_mu_cls_CB_fact = (mu_cls_CB - mu_data)**2
d2_mu_cls_CB_data = e2_mu_cls_CB_fact.sum(axis=1)
I_B_CB = (N_cls_CB*d2_mu_cls_CB_data).sum()

# Complément lien avec la variance
# Calcul de la variance des variables dans le cas où l'on remplace chaque
# individu par le centre de sa classe
d2_mu_cls_CB_fact = (mu_cls_CB - mu_data)**2
mu_cls_CB_var = (N_cls_CB @ e2_mu_cls_CB_fact)/(len(data_scaled_df) - 1)
mu_cls_CB_var_sum = mu_cls_CB_var.sum()

# == effectif - 1
print(f"Rapport entre I_B({classif_name}) et somme des variances inter = {I_B_CB/mu_cls_CB_var_sum}")
#+END_SRC

