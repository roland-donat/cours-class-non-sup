# -*- coding: utf-8 -*-
#+TITLE: TD1: Introduction et notions fondamentales
#+AUTHOR: Roland Donat
#+EMAIL: roland.donat@univ-ubs.fr
# #+DATE: 

# ==============================================
# Document Configuration
# ======================
# Orgmode
:CONFIG:
#+LANGUAGE: fr
#+OPTIONS: H:3 num:t toc:t \n:nil @:t ::t |:t ^:{} f:t TeX:t author:t d:nil timestamp:nil
#+OPTIONS: html-postamble:nil
#+STARTUP: content 
#+STARTUP: hidestars
#+DRAWERS: CONFIG OPTIONS CACHE MACROS
#+TODO: TODO(t) INPROGRESS(p) | DONE(d)
#+BIND: org-latex-table-scientific-notation "{%s}E{%s}"
:END:

# LaTeX
# -----
# Class parameters
:CONFIG:
#+LaTeX_CLASS: ubs-note
#+LaTeX_CLASS_OPTIONS: [a4paper,twoside,11pt]
#+LATEX_HEADER: \thelang{FR}
#+LATEX_HEADER: \thesubtitle{}
#+LATEX_HEADER: \institution{IUT Vannes}
#+LATEX_HEADER: \course{Classification non supervisée}
#+LATEX_HEADER: \cursus{STID 2 - 2020-2021}
#+LATEX_HEADER: \version{1.0}
:END:
# Packages
:CONFIG:
#+LATEX_HEADER: \usepackage[french]{babel}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage{amssymb}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{amsfonts}
#+LATEX_HEADER: \usepackage{xcolor}
#+LATEX_HEADER: \usepackage{verbatim}
#+LATEX_HEADER: \usepackage{tabularx}
#+LATEX_HEADER: \usepackage{float}
#+LATEX_HEADER: \usepackage{lmodern}
#+LATEX_HEADER: \usepackage{natbib}
#+LATEX_HEADER: \usepackage{subfig}
#+LATEX_HEADER: \usepackage{booktabs}
#+LATEX_HEADER: \usepackage{minted}
:END:
# Layout
:CONFIG:
# Figures path
#+LATEX_HEADER: % Graphics path
#+LATEX_HEADER: \graphicspath{ 
#+LATEX_HEADER:   {./fig/}
#+LATEX_HEADER: }

# Colors
#+LATEX_HEADER: \definecolor{almostwhite}        {rgb}{0.85,0.85,0.85}

# Minted
# To control spaces between minted block
#+LATEX_HEADER: \AtBeginEnvironment{snugshade*}{\vspace{-1.25\FrameSep}}
#+LATEX_HEADER: \AfterEndEnvironment{snugshade*}{\vspace{-2\FrameSep}}
# #+LATEX_HEADER: \usemintedstyle{monokai}
# #+LATEX_HEADER: \renewcommand{\theFancyVerbLine}{\sffamily \footnotesize {\color{EMLogoBlue}\oldstylenums{\arabic{FancyVerbLine}}}}

# Captions
#+LATEX_HEADER: \captionsetup[table]{position=bottom,margin=90pt,font=small,labelfont=bf,labelsep=endash,format=plain}
#+LATEX_HEADER: \captionsetup[figure]{position=bottom,margin=90pt,font=small,labelfont=bf,labelsep=endash,format=plain}
#+LATEX_HEADER: \captionsetup[subfloat]{margin=0pt,font=footnotesize}

# Geometry
#+LATEX_HEADER: \usepackage{geometry}
#+LATEX_HEADER: \geometry{
#+LATEX_HEADER: %  nohead,
#+LATEX_HEADER:   top=2.25cm, 
#+LATEX_HEADER:   bottom=2.25cm, 
#+LATEX_HEADER:  left=2.5cm, 
#+LATEX_HEADER:  right=2.5cm}

#+LATEX_HEADER: \usepackage{setspace}
#+LATEX_HEADER: \onehalfspacing
#+LATEX_HEADER: % Supprime l'indentation
#+LATEX_HEADER: \setlength{\parindent}{0pt}
#+LATEX_HEADER: % Espacement entre les paragraphes
#+LATEX_HEADER: \setlength{\parskip}{2ex}

# List layout
#+LATEX_HEADER: \frenchbsetup{ListOldLayout=true} %FBReduceListSpacing=true,CompactItemize=false}

# References
#+LATEX: \renewcommand*{\refname}{}*
:END:
# LaTeX Compilator
:CONFIG:
#+BEGIN_SRC emacs-lisp :results silent :tangle td1.py :exports none
(setq org-latex-listings 'minted
      org-latex-minted-options nil ;; '(("frame" "lines")))
      org-latex-pdf-process
      '("xelatex -shell-escape -interaction nonstopmode -output-directory %o %f"
        "bibtex %b"
        "xelatex -shell-escape -interaction nonstopmode -output-directory %o %f"
        "xelatex -shell-escape -interaction nonstopmode -output-directory %o %f"))
#+END_SRC
:END:

# HTML
# ----
:CONFIG:
# Org HTML Macros
#+MACRO: NEWLINE @@latex:\\@@ @@html:<br>@@
#+MACRO: HTMLFONTSIZE @@html:<font size="$2">$1</font>@@
#+MACRO: SUBTITLE @@html:<div class="slidesubtitle">$1</div>@@

# HTML options
# ------------
#+SETUPFILE: https://fniessen.github.io/org-html-themes/org/theme-readtheorg.setup
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="https://roland-donat.github.io/ubs/Charte_graphique/IUT/ubs_iut_vannes.css" />
# #+HTML_HEAD: <link rel="stylesheet" type="text/css" href="./ubs_iut_vannes.css" />
:END:

# Publishing
# ----------
:CONFIG:
#+BEGIN_SRC emacs-lisp :results silent :tangle td1.py :exports none
;; Define some export options here since in org-publish-project-alist some of them are not taken into account
;; e.g. with-toc nil
(defun my-html-export-options (plist backend)
  (cond 
    ((equal backend 'html)
     (plist-put plist :with-toc t)
     (plist-put plist :section-numbers nil)
     (plist-put plist :with-author t)
     (plist-put plist :with-email t)
     (plist-put plist :with-date t)
     ))
  plist)

(setq org-publish-project-alist
      '(
	
        ("main"
         :base-directory "./"
         :include ("rb_mod_stoch.org")
         :publishing-directory "./"
         :recursive nil
         :publishing-function org-html-publish-to-html
         :preparation-function (lambda () (setq org-export-filter-options-functions '(my-html-export-options)))
         :auto-preamble t
         :html-head  "<link rel='stylesheet' type='text/css' href='edgemind.css' />"
         :htmlized-source 
	 :section-numbers nil
         )
        ("td-html"
         :base-directory "./td/"
         :base-extension "org"
         :publishing-directory "./td"
         :recursive t
         :publishing-function org-html-publish-to-html
         :preparation-function (lambda () (setq org-export-filter-options-functions '(my-html-export-options)))
         :auto-preamble t
         :html-head  "<link rel='stylesheet' type='text/css' href='edgemind.css' />"
         :htmlized-source 
         )

	 ;; pdf
        ("td-pdf"
         :base-directory "./td/"
         :base-extension "org"
         :publishing-directory "./td"
         :recursive t
         :publishing-function org-latex-publish-to-pdf
         )

	 ("td-attach"
	 :base-directory "./td/"
	 :base-extension "xdsl\\|txt\\|csv\\|py\\|png"
         :publishing-directory "./td"
	 :recursive t
	 :publishing-function org-publish-attachment
	 )

	 ("cours-attach"
	 :base-directory "./cours/"
	 :base-extension "pdf\\|xdsl\\|txt\\|csv\\|py"
         :publishing-directory "./cours"
	 :recursive t
	 :publishing-function org-publish-attachment
	 )

        ("projet-html"
         :base-directory "./projet/"
         :base-extension "org"
         :publishing-directory "./projet"
         :recursive t
         :publishing-function org-html-publish-to-html
         :preparation-function (lambda () (setq org-export-filter-options-functions '(my-html-export-options)))
         :auto-preamble t
         :html-head  "<link rel='stylesheet' type='text/css' href='edgemind.css' />"
         :htmlized-source 
         )

	 ("projet-attach"
	 :base-directory "./projet/"
	 :base-extension "xdsl\\|txt\\|csv"
         :publishing-directory "./projet"
	 :recursive t
	 :publishing-function org-publish-attachment
	 )

	 ("css"
         :base-directory "./css/"
         :base-extension "css"
         :publishing-directory "./www/css"
         :publishing-function org-publish-attachment)
	 
	 ;("rb_mod_stoch" :components ("main" "td-pdf" "td-html" "td-attach" "cours-attach" "projet-html" "projet-attach" "css"))
	 ;("rb_mod_stoch" :components ("main" "td-pdf" "td-html" "projet-html"))
	 ("rb_mod_stoch" :components ("main"))

      ))
#+END_SRC
:END:

# ==============================================
# Document starts here
# ====================

#+BEGIN_SRC emacs-lisp :results silent :tangle td1.py :exports results
(setq td_corrige nil)
#+END_SRC

#+LATEX: \clearpage

#+ATTR_HTML: :width 50%
#+CAPTION: Image générée par [[https://openai.com/dall-e-2/][DALL-E]]
[[./vigneronnes.png]]


* À faire                                                          :noexport:
** Ajouter plus d'exemple de code python pour se lancer
- group_by
- px.scatter
- value_counts
- boucle for

* Notes                                                            :noexport:
** Difficultés côté calcul de la distance euclidienne


* Préambule

** Crédit

Ce TD reprend la trame du [[https://moodle.univ-ubs.fr/pluginfile.php/360513/mod_resource/content/2/TDR1.pdf][TD de classification non supervisée]] proposé par Mme. Arlette Antoni en
2019-2020. Le TD original était réalisé avec le logiciel =R= tandis que nous allons utiliser le
logiciel =Python=.

** Environnement logiciel

Les travaux se dérouleront sous =Python=. Si vous disposez d'une connexion internet, je vous
recommande d'utiliser une plateforme de /Notebooks/ en ligne telle que
[[https://deepnote.com][=DeepNote=]] sinon vous pouvez
toujours utiliser =Python= en local en installant la distribution [[https://www.anaconda.com/products/individual][=Anaconda=]]. L'important est de vous
assurer que vous utilisez bien la version =3.8= ou plus de =Python=.

* Introduction

L'objectif de ce TD est de mettre en pratique les principales notions vues en cours en les
appliquant sur un jeu de données réel.

Pour ce faire, nous utiliserons une adaptation des données libres [[https://archive.ics.uci.edu/ml/datasets/wine][Wine Data Set]] ne conservant que
les variables quantitatives pour la mise en oeuvre de méthodes de classification non supervisée.

Les données correspondent aux résultats d'analyses chimiques sur des vins issus d'une même région
d'Italie mais produits par trois vignerons différents. L'analyse quantifie la présence des 13
éléments suivants :  
- Alcohol
- Malic acid
- Ash
- Alcalinity of ash
- Magnesium
- Total phenols
- Flavanoids
- Nonflavanoid phenols
- Proanthocyanins
- Color intensity
- Hue
- OD280/OD315 of diluted wines
- Proline

*Les données retravaillées à utiliser dans ce TD sont [[https://roland-donat.github.io/cours-class-non-sup/td/td1/wine.csv][disponibles à ici]].*

* Modules =Python= utilisés dans ce TD

Dans ce TD, nous utiliserons les modules =Python= suivants :
- =pandas=, pour la manipulation des données ;
- =plotly=, pour les représentations graphiques ;
- =numpy=, pour utiliser des fonctions de calculs numériques "bas niveau", e.g. génération de
  nombres aléatoires ;
- =scipy=, pour utiliser d'autres fonctions de calculs numériques plus "haut niveau", e.g. calcul de
  distances.

Ces modules ne sont pas forcément installés dans votre environnement local ou en ligne. Vous pouvez
donc utiliser la commande =!pip install <nom_module>= pour les installer : 
#+attr_latex: :options bgcolor=almostwhite, frame=lines
#+BEGIN_SRC python :eval no :tangle td1.py :exports code 
!pip install pandas
!pip install plotly
!pip install scipy
#+END_SRC
 

Rappel : code d'import des modules :
#+attr_latex: :options bgcolor=almostwhite, frame=lines
#+BEGIN_SRC python :session :results silent :tangle td1.py :exports code
import pandas as pd
import numpy as np
import plotly as pl
import plotly.express as px
import scipy as sc
import scipy.spatial.distance as scd
import sklearn as sk

# Vérification des versions des librairies utilisées
{"plotly": pl.__version__, 
 "pandas": pd.__version__, 
 "numpy": np.__version__, 
 "scipy": sc.__version__}
#+END_SRC


* Chargement des données et premières analyses

1. Charger les données dans votre environnement à l'aide de la fonction =pd.read_csv=. 
   #+BEGIN_SRC python :session :results silent :tangle td1.py :exports code
data_path = "https://roland-donat.github.io/cours-class-non-sup/td/td1/wine.csv"
data_df = pd.read_csv(data_path, sep=",")
   #+END_SRC
2. Faire une rapide analyse exploratoire de vos données :
   - de manière macroscopique avec les méthodes =.head=, =.describe= et =.info= ;
   - de manière visuelle en représentant la distribution de chaque variable en utilisant la fonction
     =px.box=. 
     Que constatez-vous ?
      #+BEGIN_SRC python :session :results silent :tangle td1.py :exports code
data_df.describe()
data_df.info()
px.box(data_df, title="Boxplot de chaque variable (donnée originale)")
      #+END_SRC
3. Centrez les données. 
   #+BEGIN_SRC python :session :results silent :tangle td1.py :exports code
data_ctr_df = data_df - data_df.mean()
   #+END_SRC
4. Réduisez les données centrées précédemment. Aide : utilisez la méthode =.std= des =DataFrame=.
   #+BEGIN_SRC python :session :results silent :tangle td1.py :exports (if (eq td_corrige t) "code" "none")
# SOLUTION
# --------
data_scaled_df = data_ctr_df/data_df.std()
   #+END_SRC
5. Calculer la matrice des corrélations linéaires avec la méthode =.corr= des =DataFrame=. Quelles
   sont les caractéristiques remarquables de cette matrice ?
   #+BEGIN_SRC python :session :results silent :tangle td1.py :exports code
data_corr_df = data_df.corr()
   #+END_SRC

6. Afficher un diagramme en paire et une carte de chaleur des corrélations linéaires.
   #+BEGIN_SRC python :session :results silent :tangle td1.py :exports code
px.imshow(data_df.corr(), 
          color_continuous_midpoint=0, 
          title="Corrélations linéaires des données sous forme de carte de chaleur (heatmap)")
   #+END_SRC


* Distance euclidienne

*Dans la suite du TD, nous allons travailler avec les données centrées réduites calculées
précédemment dans le =DataFrame= =data_scaled_df=.* 

1. Calculer la distance euclidienne entre les deux premiers individus en utilisant la méthode =sum=
   des =DataFrame=. 
   #+BEGIN_SRC python :session :results silent :tangle td1.py :exports code
d2_ind_12 = ((data_scaled_df.loc[0] - data_scaled_df.loc[1])**2).sum()
d_ind_12 = d2_ind_12**(0.5)
d_ind_12
   #+END_SRC
2. Calculer la distance euclidienne entre le premier individu et le troisième, puis entre le
   deuxième et le troisième.
   #+BEGIN_SRC python :session :results silent :tangle td1.py :exports (if (eq td_corrige t) "code" "none")
# SOLUTION
# --------
d2_ind_13 = ((data_scaled_df.loc[0] - data_scaled_df.loc[2])**2).sum()
d_ind_13 = d2_ind_13**(0.5)
d_ind_13
   #+END_SRC

3. Calculer la matrice des distances euclidiennes. Aide : utilisez les fonctions =pdist= et
   =squareform= du package =scipy.spatial.distance=. Quelles sont les dimensions de cette matrice, quelles
   sont ses propriétés remarquables ?
   #+BEGIN_SRC python :session :results silent :tangle td1.py :exports code
dist_array = scd.pdist(data_scaled_df, metric="euclidean")
dist_mat = scd.squareform(dist_array)
dist_mat
   #+END_SRC
4. Transformer la matrice de distances précédente sous la forme d'un =DataFrame= pour en améliorer
   la lisibilité. Vérifier que les distances calculées aux points 1 et 2 grâce à la matrice des distances calculée au
   point 3.
   #+BEGIN_SRC python :session :results silent :tangle td1.py :exports (if (eq td_corrige t) "code" "none")
# SOLUTION
# --------
dist_mat_df = \
    pd.DataFrame(dist_mat,
                 index=data_scaled_df.index,
                 columns=data_scaled_df.index)
dist_mat_df
   #+END_SRC

* Inertie totale

*Dans la suite du TD, les données seront considérées comme étant équipondérées avec un poids de 1
pour chaque observation. Par ailleurs, la distance utilisée sera la distance euclidienne.* 

*Nous travaillons toujours avec les données centrées réduites du =DataFrame= =data_scaled_df=.* 

1. Calculer le centre de gravité du nuage d'observations. Que remarquez-vous ?
   #+BEGIN_SRC python :session :results silent :tangle td1.py :exports code
mu_data = data_scaled_df.mean()
mu_data
   #+END_SRC

2. Calculer les distances au carré entre les observations et le centre de gravité du nuage.
   #+BEGIN_SRC python :session :results silent :tangle td1.py :exports (if (eq td_corrige t) "code" "none")
# SOLUTION
# --------
d2_data_mu = ((data_scaled_df - mu_data)**2).sum(axis=1)
d2_data_mu
   #+END_SRC

3. En déduire l'inertie totale des données.
      #+BEGIN_SRC python :session :results silent :tangle td1.py :exports (if (eq td_corrige t) "code" "none")
# SOLUTION
# --------
I_T = d2_data_mu.sum()
I_T
   #+END_SRC

4. Calculer la somme des variances empiriques de chaque variable. Aide : utilisez la méthode =.var= des
   =DataFrame= ? Quels calculs fait exactement la méthode =.var= ? Aurait-on pu prévoir le résultat
   dans ce cas ?
   #+BEGIN_SRC python :session :results silent :tangle td1.py :exports (if (eq td_corrige t) "code" "none")
# SOLUTION
# --------
S2_var = data_scaled_df.var().sum()
S2_var
   #+END_SRC
   
5. Calculer le rapport entre l'inertie totale et la somme des variances de chaque
   variable. Expliquer le résultat.
   #+BEGIN_SRC python :session :results silent :tangle td1.py :exports (if (eq td_corrige t) "code" "none")
# SOLUTION
# --------
I_T/S2_var
   #+END_SRC

* Première partition $C_{\text{A}}$

** Construction des classes

Nous allons nous donner une première partition arbitraire, notée $C_{\text{A}}$, consistant à affecter :
- la classe $c_{1}$ aux individus d'indices 0-49 ;
- la classe $c_{2}$ aux individus d'indices 50-99 ;
- la classe $c_{3}$ aux individus d'indices 100-177.

*Rappel :* N'oubliez pas que =Python= indexe les listes, vecteurs, tableaux en commençant à 0 !

1. Ajoutez une nouvelle variable =cls_A= contenant la classe de chaque individu suivant le schéma
   décrit précédemment.
   #+BEGIN_SRC python :session :results silent :tangle td1.py :exports code
data_scaled_df["cls_A"] = "c1"
data_scaled_df["cls_A"].loc[50:99] = "c2"
data_scaled_df["cls_A"].loc[100:] = "c3"
   #+END_SRC
2. Visualiser le nuage d'individus sur les variables =OD280= et =Alcohol= en faisant apparaître
   votre partition. Pour ce faire, utiliser la fonction =px.scatter= avec l'option =color= pour
   colorer les individus en fonction de leur classe.
   #+BEGIN_SRC python :session :results silent :tangle td1.py :exports code
px.scatter(data_scaled_df,
           x="OD280", y="Alcohol",
           color="cls_A",
           title="Visualisation de la partition A sur les données",
           labels={"cls_A": "Partition"})
   #+END_SRC
1. Calculer l'effectif de chaque classe avec la méthode =.value_counts=.
   #+BEGIN_SRC python :session :results silent :tangle td1.py :exports code
N_cls_A = data_scaled_df["cls_A"].value_counts()
N_cls_A
   #+END_SRC

** Inertie intra-classe

1. Calculer le centre de gravité de chaque classe. Pour ce faire, utiliser la méthode =groupby= des
   =DataFrame= afin de regrouper les données sur la variable de classe et réaliser les traitements
   appropriés sur chaque groupe.
   #+BEGIN_SRC python :session :results silent :tangle td1.py :exports code
# Création des groupes selon la partition A
data_cls_A_grp = data_scaled_df.groupby("cls_A")
# Calcul des centres de chaque classe
mu_cls_A = data_cls_A_grp.mean()
mu_cls_A
   #+END_SRC
   Pour accéder au centre de la classe ="c1"=, utiliser l'accesseur =.loc= de la façon suivante =mu_cls_A.loc["c1"]=.

2. Calculez l'inertie interne de la classe ="c1"=.
   #+BEGIN_SRC python :session :results silent :tangle td1.py :exports code

# Récupération des observations de la classe "c1"
data_c1_df = data_cls_A_grp.get_group("c1")
d2_data_c1 = ((data_c1_df - mu_cls_A.loc["c1"])**2).sum(axis=1)
I_W_c1 = d2_data_c1.sum()
   #+END_SRC

3. Calculer l'inertie interne de chacune des classes de la partition A. Pour rendre votre code
   générique et réutilisable, utiliser une boucle =for=. Pour vous aider, inspirez-vous du code à compléter suivant :
   #+BEGIN_SRC python :exports code :eval no
# On initialise un vecteur de trois élements nommés c1, c2 et c3 ayant pour valeur 0.
# Ce vecteur servira à récupérer l'inertie interne des 3 groupes dans la boucle.
I_W_cls_A = pd.Series(0, index=mu_cls_A.index)
# Note : il est possible d'itérer sur un objet `groupby` avec un couple de variables.
#        Dans la boucle ci-dessous cls prendra successivement les valeurs "c1", "c2" et "c3" ;
#        et data_cls_df contiendra successivement les individus des classes "c1", "c2" et "c3". 
for cls, data_cls_df in data_cls_A_grp:
  # Calcul des distances au carré entre chaque individu de la classe courante avec le centre de cette classe.
  d2_data_cls = # À COMPLÉTER
  # Sommation des distances au carré pour obtenir l'inertie de la classe courante
  I_W_cls_A.loc[cls] = # À COMPLÉTER

I_W_cls_A
   #+END_SRC
   #+BEGIN_SRC python :session :results silent :tangle td1.py :exports (if (eq td_corrige t) "code" "none")
# SOLUTION
# --------
I_W_cls_A = pd.Series(0, index=mu_cls_A.index)
for cls, data_cls_df in data_cls_A_grp:
  d2_data_cls = ((data_cls_df - mu_cls_A.loc[cls])**2).sum(axis=1)
  I_W_cls_A.loc[cls] = d2_data_cls.sum() 

I_W_cls_A
   #+END_SRC

3. En déduire l'inertie intra-classe de la partition.
   #+BEGIN_SRC python :session :results silent :tangle td1.py :exports (if (eq td_corrige t) "code" "none")
# SOLUTION
# --------
I_W_A = I_W_cls_A.sum()
I_W_A
   #+END_SRC

4. En déduire l'inertie inter-classe de la partition.
   #+BEGIN_SRC python :session :results silent :tangle td1.py :exports (if (eq td_corrige t) "code" "none")
# SOLUTION
# --------
I_B_A = I_T - I_W_A
I_B_A
   #+END_SRC

5. Calculer le pourcentage d'inertie expliquée par la partition.
   #+BEGIN_SRC python :session :results silent :tangle td1.py :exports (if (eq td_corrige t) "code" "none")
# SOLUTION
# --------
PIE_A = 100*(1 - I_W_A/I_T)
PIE_A_bis = 100*I_B_A/I_T
(PIE_A, PIE_A_bis)
   #+END_SRC

6. Calculer la somme des variances empiriques corrigées des variables au sein de chaque classe.
   #+BEGIN_SRC python :session :results silent :tangle td1.py :exports (if (eq td_corrige t) "code" "none")
# SOLUTION
# --------
S2d_cls_A = data_cls_A_grp.var()
S2d_cls_A_sum = S2d_cls_A.sum(axis=1)
S2d_cls_A_sum
   #+END_SRC

7. Calculer le rapport des inerties internes et la somme des variances empiriques corrigées des
   variables au sein de chaque classe (résultat de la question précdente).
   #+BEGIN_SRC python :session :results silent :tangle td1.py :exports (if (eq td_corrige t) "code" "none")
# SOLUTION
# --------
I_W_cls_A/S2d_cls_A_sum
   #+END_SRC

8. Comment calculer l'inertie intra-classe en utilisant la somme des variances empiriques corrigées des
   variables au sein de chaque classe et l'effectif de chaque classe ?
   #+BEGIN_SRC python :session :results silent :tangle td1.py :exports (if (eq td_corrige t) "code" "none")
# SOLUTION
# --------
(S2d_cls_A_sum*(N_cls_A - 1)).sum()
   #+END_SRC


** Inertie inter-classe

1. Calculer le carré des distances entre le centre de gravité des classes et le centre de gravité
   des données.
   #+BEGIN_SRC python :session :results silent :tangle td1.py :exports (if (eq td_corrige t) "code" "none")
# SOLUTION
# --------
d2_mu_cls_A = ((mu_cls_A - mu_data)**2).sum(axis=1)
d2_mu_cls_A
   #+END_SRC

2. En déduire l'inertie inter-classe de la partition.
   #+BEGIN_SRC python :session :results silent :tangle td1.py :exports (if (eq td_corrige t) "code" "none")
# SOLUTION
# --------
# - On oublie pas de pondérer le calcul par le poids de chaque classe
# - Ici omega = 1 pour tous les individus, donc poids de la classe k = effectif de la classe k
I_B_A = (N_cls_A*d2_mu_cls_A).sum()
I_B_A
   #+END_SRC

* Seconde partition $C_{\text{B}}$

Reprendre les questions de la partie précédente en construisant une partition $C_{\text{B}}$ au
hasard. Pour ce faire, utilisez la fonction =np.ramdom.choice= pour affecter les classes aux
individus. N'hésitez pas à fixer la graine du générateur aléatoire avec la fonction =np.random.seed=
afin de reproduire le "même hasard" d'une exécution à l'autre de votre script. 

#+BEGIN_SRC python :session :results silent :tangle td1.py :exports code
# On fixe la graîne du générateur de nombre aléatoire pour 
# reproduire le même "hasard" d'une exécution à l'autre
np.random.seed(56)

# Initalisation au hasard de la partition
data_scaled_df["cls_B"] = np.random.choice(["c1", "c2", "c3"],
                                            len(data_scaled_df))

N_cls_B = data_scaled_df["cls_B"].value_counts()
N_cls_B
#+END_SRC
